{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Detectron2 installation**"
      ],
      "metadata": {
        "id": "zw97RgZvLsst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "!pip install CocoDataset==0.1.2\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "id": "Ye8jEPlNLtCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]"
      ],
      "metadata": {
        "id": "FGOlihqGMtCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import libraries**"
      ],
      "metadata": {
        "id": "2dSWzSR0pD7X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation class\n",
        "\n",
        "\n",
        "*   sky cropping\n",
        "\n",
        "This object design enables the addition of more augmentation methods\n"
      ],
      "metadata": {
        "id": "28hjX9e9p9eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_Augmentation:\n",
        "  \"\"\"\n",
        "  This class contains methods for data augmentation tasks\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.segmentation_pred = None\n",
        "    self.data_set = None\n",
        "\n",
        "  def __predictor_init(self):\n",
        "    \"\"\"Initializes segmentation model\"\"\"\n",
        "    cfg = get_cfg()\n",
        "    # initialize segementation model\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "    self.segmentation_pred = DefaultPredictor(cfg)\n",
        "\n",
        "  \n",
        "  def sky_crop(self, img_dict, path_to_new_images):\n",
        "    \"\"\"\n",
        "    This function inputs image annotation dictionary\n",
        "    and crops the image by the lowermost pixel which contains\n",
        "    the sky.\n",
        "    return: new updated annotation dictionary\n",
        "    \"\"\"\n",
        "    self.__predictor_init()\n",
        "    img = cv2.imread(img_dict[\"file_name\"])\n",
        "    # run inference on image\n",
        "    panoptic_seg, segments_info = self.segmentation_pred(img)[\"panoptic_seg\"]\n",
        "    # finding label corresponding to sky\n",
        "    id = 0\n",
        "    for dict in segments_info:\n",
        "      if dict['category_id'] == 40:\n",
        "        id = dict['id']\n",
        "    if id == 0:\n",
        "      print(\"no sky detected\")\n",
        "      self.__update_seg(0, img_dict['annotations'])\n",
        "      return img_dict\n",
        "    # creating mask on sky\n",
        "    panoptic_seg = panoptic_seg.cpu()\n",
        "    mask = np.where(panoptic_seg==id, 1, 0)\n",
        "    # finding max sky pixel\n",
        "    y_ind, x_ind = np.where(mask==1)\n",
        "    max_y = y_ind[-1]\n",
        "    # cropping image\n",
        "    cropped_img = img[max_y:, :, :]\n",
        "    # Save the image\n",
        "    image_name = path_to_new_images + \"/new_image\" + str(img_dict['image_id']) + \".jpg\"\n",
        "    cv2.imwrite(image_name, cropped_img)\n",
        "    img_dict['file_name'] = image_name\n",
        "    img_dict['height'] = cropped_img.shape[0]\n",
        "    img_dict['width'] = cropped_img.shape[1]\n",
        "    # fix bboxes\n",
        "    img_dict['annotations'] = self.__update_boxes(max_y, img_dict['annotations'])\n",
        "    self.__update_seg(max_y, img_dict['annotations'])\n",
        "    return img_dict\n",
        "    \n",
        "  def __update_boxes(self, max_y, boxes):\n",
        "    \"\"\"Update bounding boxes after cropping\"\"\"\n",
        "    new_dicts = []\n",
        "    # going through all dicts and for each one update bbox\n",
        "    for i in range(len(boxes)):\n",
        "      x_min, y_min, width, height = boxes[i]['bbox']  \n",
        "      if max_y <= y_min:\n",
        "        boxes[i]['bbox'][1] = int(y_min - max_y)\n",
        "        new_dicts.append(boxes[i])\n",
        "        continue\n",
        "      if max_y < y_min+height:\n",
        "        boxes[i]['bbox'][3] = int(y_min+height - max_y)\n",
        "        boxes[i]['bbox'][1] = 0\n",
        "        new_dicts.append(boxes[i])\n",
        "    return new_dicts\n",
        "\n",
        "  def __update_seg(self, max_y, boxes):\n",
        "    \"\"\"\n",
        "    Deleting segmentations for visualization reasons\n",
        "    \"\"\"\n",
        "    for i in range(len(boxes)):\n",
        "      boxes[i]['segmentation'] = []\n",
        "      \n",
        "      \n",
        "        "
      ],
      "metadata": {
        "id": "ucdE7ilgH8yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Registering COCO format dataset \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DNl5DImQlRXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "path_to_annotations = \"/content/annotations/instances_train2014.json\"\n",
        "path_to_images = \"/content/train2014\"\n",
        "dataset_name = \"My_dataset\"\n",
        "path_to_new_images = \"/content/sky_cropped\"\n",
        "path_to_new_annotations = \"/content/annotations/sky_crop_annotations.json\"\n",
        "os.makedirs(path_to_new_images, exist_ok=True)\n",
        "register_coco_instances(dataset_name, {}, path_to_annotations, path_to_images)"
      ],
      "metadata": {
        "id": "LUhLhWRlkvBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading annotations"
      ],
      "metadata": {
        "id": "5x1D3RhxqZ6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data.datasets import load_coco_json\n",
        "# Load the image and annotation data for the \"my_dataset2_train\" dataset\n",
        "dataset_dicts = load_coco_json(path_to_annotations, path_to_images, dataset_name)\n",
        "# Get the first image and annotation in the dataset\n",
        "metadata = MetadataCatalog.get(dataset_name)"
      ],
      "metadata": {
        "id": "0oX2koGt7eYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterate over all dataset"
      ],
      "metadata": {
        "id": "oNhPTN4r87O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for every dict from the list perform sky_crop then vizualize\n",
        "aug = Data_Augmentation()\n",
        "for i in range(len(dataset_dicts)):\n",
        "  curr_dict = dataset_dicts[i]\n",
        "  new_dict = aug.sky_crop(curr_dict, path_to_new_images)\n",
        "  img = cv2.imread(curr_dict['file_name'])\n",
        "  visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1)\n",
        "  out = visualizer.draw_dataset_dict(curr_dict)\n",
        "  cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "Xza-_IFksLlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving json"
      ],
      "metadata": {
        "id": "0S3Aiu7_M2-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_to_new_annotations, \"w\") as f:\n",
        "    # Write the list of dictionaries to the file as JSON\n",
        "    json.dump(dataset_dicts, f)"
      ],
      "metadata": {
        "id": "wz35gxtYM1_m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}